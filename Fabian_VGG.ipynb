{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Google Colab to import Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install opencv-python\n",
    "#%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JPG to JPEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "DATA_DIR = \"dataset/AI Art vs Real Art/\" # Put unzipped files to the unzipped folder and change accordingly\n",
    "\n",
    "## For Google Colab\n",
    "# DATA_DIR = \"drive/MyDrive/dataset/AI Art vs Real Art/\" # Put unzipped files to the unzipped folder and change accordingly\n",
    "\n",
    "def jpg_to_jpeg(data_dir):\n",
    "    for dir_name in os.listdir(data_dir):\n",
    "        files = os.path.join(data_dir,dir_name)\n",
    "        for filepaths in os.listdir(files):\n",
    "            file_names = os.path.join(files,filepaths)\n",
    "            if file_names.endswith(\".jpg\") or file_names.endswith(\".JPG\"):\n",
    "                img = cv2.imread(str(file_names))\n",
    "                cv2.imwrite(file_names[0:-4]+\".jpeg\", img)\n",
    "                os.remove(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imghdr\n",
    "\n",
    "IMAGE_EXTENSIONS = [\".png\", \".jpg\", \".jpeg\"]  # add there all your images file extensions\n",
    "\n",
    "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
    "for filepath in Path(DATA_DIR).rglob(\"*\"):\n",
    "    if filepath.suffix.lower() in IMAGE_EXTENSIONS:\n",
    "        img_type = imghdr.what(filepath)\n",
    "        if img_type is None:\n",
    "            print(f\"{filepath} is not an image\")\n",
    "            os.remove(filepath)\n",
    "        elif img_type not in img_type_accepted_by_tf:\n",
    "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "IMAGE_SIZE = (HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    label_mode='binary',\n",
    "    labels='inferred',\n",
    "    color_mode='rgb',\n",
    "    directory=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    seed=1337,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "val_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    label_mode='binary',\n",
    "    labels='inferred',\n",
    "    color_mode='rgb',\n",
    "    directory=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    seed=1337,\n",
    "    validation_split=0.2, # use 20% as validation\n",
    "    subset=\"validation\",\n",
    ")\n",
    "# The target labels\n",
    "class_names = train_dataset.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle the training dataset\n",
    "train_dataset = train_dataset.shuffle(len(train_dataset), seed=1337)\n",
    "\n",
    "# Separate images and labels\n",
    "images = []\n",
    "labels = []\n",
    "for image_batch, label_batch in train_dataset:\n",
    "    images.extend(image_batch)\n",
    "    labels.extend(label_batch)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Find the class with fewer samples\n",
    "unique_labels, class_counts = np.unique(labels, return_counts=True)\n",
    "minority_class = unique_labels[np.argmin(class_counts)]\n",
    "\n",
    "# Find the indices of the minority class\n",
    "minority_indices = np.where(labels == minority_class)[0]\n",
    "\n",
    "# Randomly sample from the majority class\n",
    "majority_indices = np.where(labels != minority_class)[0]\n",
    "num_samples_to_keep = len(minority_indices)\n",
    "majority_indices_sampled = np.random.choice(majority_indices, size=num_samples_to_keep, replace=False)\n",
    "\n",
    "# Combine indices of both classes\n",
    "balanced_indices = np.concatenate([minority_indices, majority_indices_sampled])\n",
    "\n",
    "# Shuffle the combined indices\n",
    "balanced_indices = shuffle(balanced_indices, random_state=1337)\n",
    "\n",
    "# Use the balanced indices to create the balanced dataset\n",
    "balanced_images = images[balanced_indices]\n",
    "balanced_labels = labels[balanced_indices]\n",
    "\n",
    "# Create TensorFlow Dataset from balanced data\n",
    "balanced_train_dataset = tf.data.Dataset.from_tensor_slices((balanced_images, balanced_labels))\n",
    "balanced_train_dataset = balanced_train_dataset.shuffle(len(balanced_images)).batch(BATCH_SIZE)\n",
    "\n",
    "# Print class distribution in the balanced dataset\n",
    "unique_labels, class_counts = np.unique(balanced_labels, return_counts=True)\n",
    "print(\"Class distribution in the balanced dataset:\")\n",
    "for label, count in zip(unique_labels, class_counts):\n",
    "    print(f\"Class {label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_binary_images(dataset, class_names):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title(class_names[int(labels[i].numpy())])\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "display_binary_images(train_dataset, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20  # Number of epochs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the VGG16 model pre-trained on ImageNet data\n",
    "base_model = tf.keras.applications.VGG16(input_shape=(HEIGHT, WIDTH, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze the layers in the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "predictions = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Combine the base model with the custom classification head\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Lower learning rate for fine-tuning\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Learning Rate Scheduling callback\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    factor=0.2,           # Factor by which the learning rate will be reduced\n",
    "    patience=3,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1             # Print message when learning rate is reduced\n",
    ")\n",
    "\n",
    "# Define Early Stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=5,          # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,           # Print message when training is stopped\n",
    "    restore_best_weights=True  # Restore model weights to the point of best validation loss\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(balanced_train_dataset,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[lr_scheduler, early_stopping]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    if layer.name.startswith('conv2') or layer.name.startswith('conv4'):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-6),  # Very low learning rate\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(balanced_train_dataset,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[lr_scheduler, early_stopping]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unfreeze some layers in the base model\n",
    "# # Here, we unfreeze the last convolutional block\n",
    "# for layer in base_model.layers:\n",
    "#     if layer.name.startswith('conv5'):\n",
    "#         layer.trainable = True\n",
    "#     else:\n",
    "#         layer.trainable = False\n",
    "\n",
    "# Unfreeze specific layers in the base model\n",
    "# Here, we unfreeze the middle, and last convolutional blocks\n",
    "for layer in base_model.layers:\n",
    "    if layer.name.startswith('conv3') or layer.name.startswith('conv5'):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-6),  # Very low learning rate\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(balanced_train_dataset,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[lr_scheduler, early_stopping]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# It's important to recompile your model after you make any changes\n",
    "# to the `trainable` attribute of any inner layer, so that your changes\n",
    "# are take into account\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-8),  # Very low learning rate\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(balanced_train_dataset,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[lr_scheduler, early_stopping]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evalutation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
